<html>
	<head>
	
		<style type="text/css">
			h1 {color: blue;text-align: left;font-size: 24px;font-weight: bold;}
			p {color: red;font-size: 16px;text-align: justify;}
			body{font-family: sans-serif;background-image: url('back.png');background-size: cover;background-attachment: fixed;}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-weight: bold;color: SpringGreen;font-size: 32px;padding: 1%;}
			.section{position: relative;width: 90%;color:black;margin: auto;padding: 2%;}
			.subsection{position: relative; width: 98%;color: white;text-align: justify;padding: 10px;}
			.heading{position: relative; width: 98%;text-align: left;font-size: 28px;color: white;font-weight: bold;}
			.text{width: 95%;font-size: 16px;color: white;text-align: justify;padding: 10px 0px 10px 0px;}
			.authors{position: relative;width: 80%;margin: auto;padding: 2%;font-style: italic;text-align: center;color: cyan;font-size: 14px;}
			.image{width: 95%;font-size: 14px;text-align: left;}
			
		</style>
	</head>
	<body>
		<div class="container">
			<div class="title">MUSICAL INSTRUMENT IDENTIFICATION</div>

			<div class="authors">

				<!-- Start edit here  -->
				Kunal Dhawan, Roll No.: 150102030, Branch: ECE; &nbsp; &nbsp;
				N Jayanth Kumar Reddy, Roll No.: 150102035, Branch: ECE; &nbsp; &nbsp;
				Nagre Amar Sheshrao, Roll No.: 150102037, Branch: ECE; &nbsp; &nbsp;
				Shubham, Roll No.: 150102079, Branch: ECE; &nbsp; &nbsp;
				Kumar Priyadarshi, Roll No.: 150102074, Branch: ECE; &nbsp; &nbsp;
				<!-- Stop edit here -->

			</div>


			<div class="section">
				<h1><div class="heading"><p2 style="background-color:MediumSeaGreen;">Abstract</p2></div></h1></p>
				<div class="text">

					<!-- Start edit here  -->
					<!--is the place where you have to write your abstract. Write a brief description about your work here. Mention the features that you have used in your work and their respective motivations.-->
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<h1><div class="heading"><p2 style="background-color:MediumSeaGreen;">1. Introduction</p2></div></h1>
				<div class="text">

					<!-- Start edit here  -->
		
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading"><h1>1.1 Introduction to Problem</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						Musical Instrument Identification by a computer has many scientific as well as practical applications such as automatic annotation of musical data, structured coding and ultimately developing a system that can understand music 
					enough to collaborate with a human in real-time performance.The goal of this project is to identify musical instruments from their audio samples using a statistical pattern-recognition technique.
						<!-- Stop edit here -->

					</div>
				</div>
				
				<div class="subsection">
					<div class="heading"><h1>1.2 Motivation</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						There are many scientific and practical applications in which musical instrument identification by  a computer would be useful.  Some of them are :
						<ul>
							<li>Automatically annotating musical multimedia data</li>
							<li>Transcribing musical performances for purposes of teaching, theoretical study, or structured coding</li>
							<li>Ultimately, developing a system that can understand music enough to collaborate with a human in real-time performance</li>
						</ul>
						By attempting to build such systems, we stand to learn a great deal about the human system we seek to emulate.

						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading"><h1>1.3 Figure</h1></div>
					<div class="image">

						<!-- Start edit here  -->
						<br>Block diagram of the Musical Instrument Identification approach <br>
						<img src="Picture.png" alt="This text displays when the image is umavailable" width="680px" height=""/>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading"><h1>1.4 Literature Review</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						[1] Giulio Agostini,Maurizio Longari and Emanuele Pollastri,"Musical Instrument Timbres Classification with Spectral Features",<i>EURASIP Journal on Applied Signal Processing 2003</i>.
						<br><br>This paper addresses the problem of musical instrument classification from audio sources. A detailed analysis of spectral features and their precise description is presented along with their relative salience in the classification process. A number of classifying methods such as Discriminant Analysis techniques, Support Vector Machines and k-nearest neighbours have been tested and compared.
						<br><br><br>[2] Keith D. Martin and Youngmoo E. Kim, "Musical instrument identification: A pattern-recognition approach",<i>Presented at the 136 th meeting of the Acoustical Society of America, October 13, 1998</i>, MIT Media Lab Machine Listening Group
Rm. E15-401, 20 Ames St., Cambridge, MA 02139. 
						<br><br>The paper emphatically demonstrates the utility of a hierarchical organization of
musical instrument sounds. It also demonstrates that the acoustic properties studied in the literature
as components of musical timbre are indeed useful features for musical instrument recognition. Starting from basic features, it follows a taxonomical hierarchy in classifying the instruments. Prominent features have also been discussed which influenced the classification process among the instruments belonging to a single family.
						<br><br><br>[3] Antti Eronen and Anssi Klapuri, "Musical Instrument Recognition using Cepstral Coefficients and temporal features",<i>Signal Processing Laboratory </i> , Tampere University of Technology
P.O.Box 553, FIN-33101 Tampere, FINLAND
						<br><br>In this paper, a system for pitch-independent musical instrument recognition is presented. A wide set of features covering both spectral and temporal properties of sounds was investigated, and their extraction algorithms were designed. Very high accuracy was attained with a certain set of features. Also, utilization of a hierarchical classification framework is considered.
						<!-- Stop edit here -->
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading"><h1>1.5 Proposed Approach</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						Why do different musical instruments have different sounds?<br>
Various characteristics of sound, such as loudness (related to energy) and pitch (related to frequency) determine how our brain perceives a musical instrument.
BUT, if a clarinet and a piano play notes of the same pitch and loudness, the sounds will still be quite distinct to our ears.  
What , then , discriminates the sound of a clarinet from a violin ?<br><br>

The answer is Timbre!<br>
Timbre distinguishes different types of  musical instruments, such as string instruments, wind instruments, and percussion instruments. It also enables listeners to distinguish different instruments in the same category (e.g.  a clarinet and an oboe) .<br>
Musical instruments do not vibrate at a single frequency: a given note involves vibrations at many different frequencies, often called harmonics,  partials, or overtones. The relative pitch and loudness of these overtones along with other acoustic features give the note a characteristic sound we call the timbre of the instrument.<br><br>

Timbre can be successfully used to distinguish between the 4 broad classes of music. After that we may use class-specific features to distinguish the intrument within the class.<br><br>

Thus to achieve the goal of detecting the instrument bein played in a given sound clip, we propose a two step approach: <br>
<br>[1] In the first step we take advantage of the fact that musical instruments can be divided into 3 broad classes: Woodwind; String and Brass. Thus we train use a simple multilayer perceptron trained using basic timbre features : Spectral Centroid, Zero Crossing rate and Spectral Rolloff. This helps find the basic class of the given musical instrument and leaves us with the task of finding the exact instrument from within that basic class.<br>
<br>[2] In the second step, we train a classifer with individual class specific features . This helps us to accuarately identify the musical intrument within the basic class as detected earlier.<br>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading"><h1>1.6 Report Organization</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						This project report is organized as follows. <br>
						<br>[1] First, we give some background information on the importance of automatic Musical Instrument recognition and the motivation behind it.<br>
						<br>[2] This is followed by the section on proposed approach where we discuss the approach towards the classification task. Some details about feature properties and their mathematical constructs are also presented.<br>
						<br>[3] Then, a brief description of the dataset is followed by classification techniques employed in the project. <br>
						<br>[4] Finally, results are presented along with the conclusion of the project. A short summary and possible future extensions close the report.
					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading"><p2 style="background-color:MediumSeaGreen;">2. Proposed Approach</p2></div>
				<div class="text">

					<!-- Start edit here  -->
					The approach towards identification of musical instruments relies on the set of features, collectively known as "Timbre". 
					There are two broad types of features of a music signal specifying its properties:<br>
<ul>
<li>The temporal features (time domain features), which are simple to extract and have easy physical interpretation, like : the energy of signal, zero crossing rate, maximum amplitude, minimum energy, etc.</li>
<li>The spectral features (frequency based features), which are obtained by converting the time based signal into the frequency domain using the Fourier Transform, like: fundamental frequency,  frequency components, spectral centroid, etc.</li>
</ul>
   Timbre depends primarily upon the spectral features,although it also depends upon the sound pressure and the temporal characteristics of the sound. Some of the prominent features are described below :<br>
					<br>[1] <p><u><b>Amplitude Envelope:-</b></u></p> refers to the changes in the amplitude of a sound over time, and is an influential property as it affects our perception of timbre.<br>
					<br>[2] <p><u><b>Zero-crossing rate:-</b></u></p> The zero-crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to negative or back.This is a good measure of the pitch as well as the noisiness of a signal.This feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.<br>
					<br><img src="zcr.PNG" alt="This text displays when the image is umavailable"/>
					where <b>s</b> is a signal of length T and <b>1</b> is an indicator function.<br>.
					<br>[3] <p><u><b>Pitch</b></u></p> is a perceptual property of sounds that allows their ordering on a frequency-related scale. Pitch may be quantified by frequency; "high" pitch means very rapid oscillation, and "low" pitch corresponds to slower oscillation. <br>
					<br>[4] <p><u><b>Spectral Envelope:-</b></u></p> Spectral envelope is a curve in the frequency-amplitude plane, derived from a fourier magnitude spectrum. It wraps tightly and smoothly around the magnitude spectrum ,linking the peaks.<br>
					<br>[5] <p><u><b>Spectral Centroid:-</b></u></p> Spectral Centroid is simply the centroid of the spectral envelope, and is one of the most important attributes governing timbre.<br>
					<br><img src="centroid.png" alt="This text displays when the image is umavailable"/>
					<br>[6] <p><u><b>Spectral Rolloff:-</b></u></p> This is a measure of the amount of the right-skewedness of the power spectrum.
The spectral rolloff point is the fraction of bins in the power spectrum at which 85% of the power is at lower frequencies<br>
					<br>[7] <p><u><b>Intensity:-</b></u></p>The sum of the energy in the spectral envelope approximates the instantaneous loudness of the signal. Tracking this over time leads to simple measures of amplitude modulation, which can reveal tremolo (an important feature for brass instruments). Intensity is the basis of many other spectral
					features like <b>Spectral Rolloff frequency</b> among others.<br>
					<br>[8] <p><u><b>Spectral Crest:-</b></u></p>It is defined as ratio of peak and rms value of the spectrum. It is usually expressed in dB , thus it's alternatively defined as the level difference between the peak and the RMS value of the waveform. Most ambient noise has a crest factor of around 10 dB while impulsive sounds such as gunshots can have crest factors of over 30 dB.<br>
					<br><img src="crestformula.png" alt="This text displays when the image is umavailable"/>
					<br>[9] <p><u><b>Spectral flatness or tonality coefficient:-</b></u></p>( also popularly known as Wiener entropy) is a measure used in digital signal processing to quantify how noise-like a sound is, as opposed to being tone-like. The meaning of tonal in this context is in the sense of the amount of peaks or resonant structure in a power spectrum, as opposed to flat spectrum of a white noise. A high spectral flatness (approaching 1.0 for white noise) indicates that the spectrum has a similar amount of power in all spectral bands — this would sound similar to white noise, and the graph of the spectrum would appear relatively flat and smooth. A low spectral flatness (approaching 0.0 for a pure tone) indicates that the spectral power is concentrated in a relatively small number of bands — this would typically sound like a mixture of sine waves, and the spectrum would appear "spiky".</br>
					<br><img src="flatness.png" alt="This text displays when the image is umavailable"/>
					<br>[10] <p><u><b>The skewness of a spectrum:-</b></u></p> The skewness of a spectrum is the third central moment of this spectrum, divided by the 1.5 power of the second central moment.</br>
					<br>[11] <p><u><b>Spectral Slope:-</b></u></p> is a measure of how quickly the spectrum of an audio sound tails off towards the high frequencies. One way to quantify this is by applying linear regression to the Fourier magnitude spectrum of the signal, which produces a single number indicating the slope of the line-of-best-fit through the spectral data.</br>
					<br>[12] <p><u><b>Spectral leakage/Decrease:-</b></u></p> Any linear time-invariant operation on s(t) produces a new spectrum of the form H(f)•S(f), which changes the relative magnitudes and/or angles (phase) of the non-zero values of S(f). Any other type of operation creates new frequency components that may be referred to as spectral leakage. The spectrum of a product is the convolution between S(f) and another function, which inevitably creates the new frequency components. But the term 'leakage' usually refers to the effect of windowing, which is the product of s(t) with a different kind of function, the window function. Window functions happen to have finite duration, but that is not necessary to create leakage. Multiplication by a time-variant function is sufficient.<br>
					<br>[13] <p><u><b>Variance of Spectral Centroid:-</b></u></p> Its the variance of the spectral centroid over our signal.This is a useful feature as it tells the very nature of our spectral spead over the frequency range.<br>
					<br>[14] <p><u><b>The mel-frequency cepstrum (MFC) :-</b></u></p> is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. The difference between the normal cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. This frequency warping can allow for better representation of sound</br>
					<u><br>Step by Step Explanation of MFCC Feature Extraction:-</u><br><br>
1.	Frame the signal into short frames.<br>
2.	For each frame calculate the periodogram estimate of the power spectrum.<br>
3.	Apply the mel filterbank to the power spectra, sum the energy in each filter.<br>
4.	Take the logarithm of all filterbank energies.<br>
5.	Take the DCT of the log filterbank energies.<br>
6.	Keep DCT coefficients 2-13, discard the rest.<br>
	<u><br><b>Intuitive Understanding of Each step in MFCC feature extraction :-</u></b><br><br>
An audio signal is constantly changing, so to simplify things we assume that on short time scales the audio signal doesn't change much (when we say it doesn't change, we mean statistically i.e. statistically stationary, obviously the samples are constantly changing on even short time scales). This is why we frame the signal into 20-40ms frames. If the frame is much shorter we don't have enough samples to get a reliable spectral estimate, if it is longer the signal changes too much throughout the frame.
The next step is to calculate the power spectrum of each frame. This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. Depending on the location in the cochlea that vibrates (which wobbles small hairs), different nerves fire informing the brain that certain frequencies are present. Our periodogram estimate performs a similar job for us, identifying which frequencies are present in the frame.
The periodogram spectral estimate still contains a lot of information not required for Automatic Speech Recognition (ASR). In particular the cochlea can not discern the difference between two closely spaced frequencies. This effect becomes more pronounced as the frequencies increase. For this reason we take clumps of periodogram bins and sum them up to get an idea of how much energy exists in various frequency regions. This is performed by our Mel filterbank: the first filter is very narrow and gives an indication of how much energy exists near 0 Hertz. As the frequencies get higher our filters get wider as we become less concerned about variations. We are only interested in roughly how much energy occurs at each spot. The Mel scale tells us exactly how to space our filterbanks and how wide to make them. 
Once we have the filterbank energies, we take the logarithm of them. This is also motivated by human hearing: we don't hear loudness on a linear scale. Generally to double the percieved volume of a sound we need to put 8 times as much energy into it. This means that large variations in energy may not sound all that different if the sound is loud to begin with. This compression operation makes our features match more closely what humans actually hear. Why the logarithm and not a cube root? The logarithm allows us to use cepstral mean subtraction, which is a channel normalisation technique.
The final step is to compute the DCT of the log filterbank energies. There are 2 main reasons this is performed. Because our filterbanks are all overlapping, the filterbank energies are quite correlated with each other. The DCT decorrelates the energies which means diagonal covariance matrices can be used to model the features in e.g. a HMM classifier. But notice that only 12 of the 26 DCT coefficients are kept. This is because the higher DCT coefficients represent fast changes in the filterbank energies and it turns out that these fast changes actually degrade ASR performance, so we get a small improvement by dropping them.<br>
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading"><p2 style="background-color:MediumSeaGreen;">3. Experiments &amp; Results</p2></div>
				<div class="subsection">
					<div class="heading"><h1>3.1 Dataset Description</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						<br>The dataset chosen for the task at hand is the audio samples taken from the University of Iowa - Electronic Music Studios .
						The samples taken belong to the three instrument families namely - Brass, String and Woodwind.<br>
						<br><u>Instruments used were:-</u><br><br>
						STRING : <br>1)Viola - 100 samples<br>
2)Violin - 90 samples<br><br>
BRASS : <br>1)Trumpet - 71 samples<br>
2)TenorTrombone - 33 samples<br>
3)Tuba - 37 samples<br>
4)Horn - 44 samples<br><br>
WOODWIND : <br>1)Saxophone - 32 samples<br>
2)EbClarinet - 39 samples<br>
3)Oboe - 35 samples<br>
4)Flute - 77 samples<br><br>
						There are a total of 558 samples of which a 80 % - 20 % division has been done to be used for training and testing, respectively.<br>
						<br> The link to the dataset is here : <a href="https://drive.google.com/open?id=14ElVjvVGH436WP9aaYPSTCBAW1jvNRjN"> University of Iowa : Audio Samples Site</a>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading"><h1>3.2 Discussion</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						<br><p><u><b>INDIVIDUAL TRAINING:-</b></u></p><br><br><u><b>Basic family classifier</b></u><br><br>
Training - 446<br>
Testing - 112<br>
	<br><img src="1.png" alt="This text displays when the image is umavailable"/><br>
	<br>Training accuracy = 97.76%<br>
Testing accuracy = 96.42%<br>
	<br><u><b>Brass classifier</b></u><br><br>
	Training - 148<br>
Testing - 37<br>

	<br><img src="2.png" alt="This text displays when the image is umavailable"/><br>
	<br>Training accuracy = 100%<br>
Testing accuracy = 100%<br>
	<br><u><b>String classifier</b></u><br><br>
	Training - 152<br>
Testing - 38<br>
	<br><img src="3.png" alt="This text displays when the image is umavailable"/><br>
	<br>Training accuracy = 96.7%<br>
Testing accuracy = 100%<br>
	<br><u><b>Woodwind classifier</b></u><br><br>
	Training - 146<br>
Testing - 37<br>
	<br><img src="4.png" alt="This text displays when the image is umavailable"/><br>
	<br>Training accuracy = 99.3%<br>
Testing accuracy = 94.59%<br>
	<br>6 layer Neural Networks with different hyperparameters have been used for classifiers.<br>
	<br><p><u><b>Features used in individual classifiers:-</b></u></p><br>
	<br><u><b>Basic classifier and brass classifier:-</b></u><br><br>
1. Spectral centroid<br>
2. Zero crossing<br>
3. Spectral rollof<br>
4. Spectral Crest<br>
5. Spectral Decrease<br>
6. Spectral Flatness<br>
7. Spectral Skewness<br>
8. Spectral Slope<br>
<br><u><b>String classifier:-</b></u><br><br>
1. Spectral Centroid<br>
2. Fundamental frequency<br>
3. Spectral centroid variance<br>
4. Spectral Crest<br>
5. Spectral Decrease<br>
6. Spectral Flatness<br>
7. Spectral Skewness<br>
8. Spectral Slope<br>
<br><u><b>Woodwind classifier :-</b></u><br><br>
1. Spectral Centroid<br>
2. Spectral Crest<br>
3. Spectral Decrease<br>
4. Spectral Flatness<br>
5. Spectral Skewness<br>
6. Spectral Slope<br>
7. Fundamental Frequency<br>
8. Variance of Spectral Centroid<br>
9. MFCC<br>
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading"><p2 style="background-color:MediumSeaGreen;">4. Conclusions</p2></div>
				<div class="subsection">
					<div class="heading"><h1>4.1 Summary</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						<br>As proposed in our approach, we were successfully able to build a two stage musical instrument detector. We were able to achieve decent test accuracy ( 96.42% for basic classifier, 100% for brass family , 96.7% for string family and 94.59% for the woodwind family). This shows that the features we had chosen for each of the classifiers based on the unique traits of each class were quite appropriate. We have also tried to ensure minimal use of inbuilt MATLAB functions and have written code to the features and supporting functions( like spectrogram, Radix 2 FFT,etc) too ourselves using the algorithms discussed in class (EE320- Digital Signal Processing) . <br>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading"><h1>4.2 Future Extensions</h1></div>
					<div class="text">

						<!-- Start edit here  -->
						Next aim is to work on Indian musical instrument classification.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

		</div>
	</body>
</html>
