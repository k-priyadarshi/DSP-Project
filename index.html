<html>
	<head>
		
		<style type="text/css">
			body{font-family: sans-serif;background-image: "back.png";backgound-size: cover;background-attachment: fixed;}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-weight: bold;font-size: 20px;padding: 1%;}
			.section{position: relative;width: 90%;margin: auto;padding: 2%;}
			.subsection{position: relative; width: 98%;text-align: justify;padding: 10px;}
			.heading{position: relative; width: 98%;text-align: left;font-size: 16px;font-weight: bold;}
			.text{width: 95%;font-size: 14px;text-align: justify;padding: 10px 0px 10px 0px;}
			.authors{position: relative;width: 80%;margin: auto;padding: 2%;font-style: italic;text-align: center;font-size: 14px;}
			.image{width: 95%;font-size: 14px;text-align: left;}
		</style>
	</head>
	<body>
		<div class="container">
			<div class="title">MUSICAL INSTRUMENT IDENTIFICATION</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>Kunal Dhawan, Roll No.: 150102030, Branch: ECE</p>; &nbsp; &nbsp;
				<p>N Jayanth Kumar Reddy, Roll No.: 150102035, Branch: ECE</p>; &nbsp; &nbsp;
				<p>Nagre Amar Sheshrao, Roll No.: 150102037, Branch: ECE</p>; &nbsp; &nbsp;
				<p>Shubham, Roll No.: 150102079, Branch: ECE</p>; &nbsp; &nbsp;
				<p>Kumar Priyadarshi, Roll No.: 150102074, Branch: ECE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->

			</div>


			<div class="section">
				<div class="heading">Abstract</div>
				<div class="text">

					<!-- Start edit here  -->
					<!--is the place where you have to write your abstract. Write a brief description about your work here. Mention the features that you have used in your work and their respective motivations.-->
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->
		
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading">1.1 Introduction to Problem</div>
					<div class="text">

						<!-- Start edit here  -->
						Musical Instrument Identification by a computer has many scientific as well as practical applications such as automatic annotation of musical data, structured coding and ultimately developing a system that can understand music 
					enough to collaborate with a human in real-time performance.The goal of this project is to identify musical instruments from their audio samples using a statistical pattern-recognition technique.
						<!-- Stop edit here -->

					</div>
				</div>
				
				<div class="subsection">
					<div class="heading">1.2 Motivation</div>
					<div class="text">

						<!-- Start edit here  -->
						There are many scientific and practical applications in which musical instrument identification by  a computer would be useful.  Some of them are :
						<ul>
							<li>Automatically annotating musical multimedia data</li>
							<li>Transcribing musical performances for purposes of teaching, theoretical study, or structured coding</li>
							<li>Ultimately, developing a system that can understand music enough to collaborate with a human in real-time performance</li>
						</ul>
						By attempting to build such systems, we stand to learn a great deal about the human system we seek to emulate.

						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">1.3 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
						<br>Block diagram of the Musical Instrument Identification approach <br>
						<img src="Picture.png" alt="This text displays when the image is umavailable" width="680px" height=""/>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.4 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						[1] Giulio Agostini,Maurizio Longari and Emanuele Pollastri,"Musical Instrument Timbres Classification with Spectral Features",<i>EURASIP Journal on Applied Signal Processing 2003</i>.
						
						<br><br>[2] Keith D. Martin and Youngmoo E. Kim, "Musical instrument identification: A pattern-recognition approach",<i>Presented at the 136 th meeting of the Acoustical Society of America, October 13, 1998</i>, MIT Media Lab Machine Listening Group
Rm. E15-401, 20 Ames St., Cambridge, MA 02139. 
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.5 Proposed Approach</div>
					<div class="text">

						<!-- Start edit here  -->
						Why do different musical instruments have different sounds?<br>
Various characteristics of sound, such as loudness (related to energy) and pitch (related to frequency) determine how our brain perceives a musical instrument.
BUT, if a clarinet and a piano play notes of the same pitch and loudness, the sounds will still be quite distinct to our ears.  
What , then , discriminates the sound of a clarinet from a piano ?<br><br>

The answer is Timbre!<br>
Timbre distinguishes different types of  musical instruments, such as string instruments, wind instruments, and percussion instruments. It also enables listeners to distinguish different instruments in the same category (e.g.  a clarinet and an oboe) .<br>
Musical instruments do not vibrate at a single frequency: a given note involves vibrations at many different frequencies, often called harmonics,  partials, or overtones. The relative pitch and loudness of these overtones along with other acoustic features give the note a characteristic sound we call the timbre of the instrument.<br><br>

Timbre can be successfully used to distinguish between the 4 broad classes of music. After that we may use class-specific features to distinguish the intrument within the class.<br><br>

Thus to achieve the goal of detecting the instrument bein played in a given sound clip, we propose a two step approach: <br>
[1] In the first step we take advantage of the fact that musical instruments can be divided into 4 broad classes: Woodwind; String; Keyboard; Brass. Thus we train use a simple multilayer perceptron trained using basic timbre features : Spectral Centroid, Zero Crossing rate and Spectral Rolloff. This helps find the basic class of the given musical instrument and leaves us with the task of finding the exact instrument from within that basic class.<br>
[2] In the second step, we train a classifer with individual class specific features . This helps us to accuarately identify the musical intrument within the basic class as detected earlier.<br>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.6 Report Organization</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">2. Proposed Approach</div>
				<div class="text">

					<!-- Start edit here  -->
					The approach towards identification of musical instruments relies on the set of features, collectively known as "Timbre". 
					There are two broad types of features of a music signal specifying its properties:<br>
<ul>
<li>The temporal features (time domain features), which are simple to extract and have easy physical interpretation, like : the energy of signal, zero crossing rate, maximum amplitude, minimum energy, etc.</li>
<li>The spectral features (frequency based features), which are obtained by converting the time based signal into the frequency domain using the Fourier Transform, like: fundamental frequency,  frequency components, spectral centroid, etc.</li>
</ul>
   Timbre depends primarily upon the spectral features,although it also depends upon the sound pressure and the temporal characteristics of the sound. Some of the prominent features are described below :<br>
					[1] <b>Amplitude Envelope</b> refers to the changes in the amplitude of a sound over time, and is an influential property as it affects our perception of timbre.<br>
					<br>[2] The <b>zero-crossing rate:-</b> The zero-crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to negative or back.This is a good measure of the pitch as well as the noisiness of a signal.This feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.<br>
					<img src="zcr.PNG" alt="This text displays when the image is umavailable"/>
					where <b>s</b> is a signal of length T and <b>1</b> is an indicator function.<br>.
					<br>[3] <b>Pitch</b> is a perceptual property of sounds that allows their ordering on a frequency-related scale. Pitch may be quantified by frequency; "high" pitch means very rapid oscillation, and "low" pitch corresponds to slower oscillation. <br>
					<br>[4]A <b>Spectral Envelope</b> is a curve in the frequency-amplitude plane, derived from a fourier magnitude spectrum. It wraps tightly and smoothly around the magnitude spectrum ,linking the peaks.<br>
					<br>[5] The <b>Spectral Centroid</b> is simply the centroid of the spectral envelope, and is one of the most important attributes governing timbre.<br>
					<img src="centroid.png" alt="This text displays when the image is umavailable"/>
					<br>[6]<b>Spectral Roll off:-</b> This is a measure of the amount of the right-skewedness of the power spectrum.
The spectral rolloff point is the fraction of bins in the power spectrum at which 85% of the power is at lower frequencies<br>
					<br>[7] <b>Intensity</b> – The sum of the energy in the spectral envelope approximates the instantaneous loudness of the signal. Tracking this over time leads to simple measures of amplitude modulation, which can reveal tremolo (an important feature for brass instruments). Intensity is the basis of many other spectral
					features like <b>Spectral Rolloff frequency</b> among others.<br>
					<br>[8]<b>Spectral Crest</b>It is defined as ratio of peak and rms value of the spectrum. It is usually expressed in dB , thus it's alternatively defined as the level difference between the peak and the RMS value of the waveform. Most ambient noise has a crest factor of around 10 dB while impulsive sounds such as gunshots can have crest factors of over 30 dB.<br>
					<img src="crestformula.png" alt="This text displays when the image is umavailable"/>
					<br>[9]<b>Spectral flatness or tonality coefficient </b>( also popularly known as Wiener entropy) is a measure used in digital signal processing to quantify how noise-like a sound is, as opposed to being tone-like. The meaning of tonal in this context is in the sense of the amount of peaks or resonant structure in a power spectrum, as opposed to flat spectrum of a white noise. A high spectral flatness (approaching 1.0 for white noise) indicates that the spectrum has a similar amount of power in all spectral bands — this would sound similar to white noise, and the graph of the spectrum would appear relatively flat and smooth. A low spectral flatness (approaching 0.0 for a pure tone) indicates that the spectral power is concentrated in a relatively small number of bands — this would typically sound like a mixture of sine waves, and the spectrum would appear "spiky".</br>
					<img src="flatness.png" alt="This text displays when the image is umavailable"/>
					<br>[10]<b>The skewness of a spectrum</b> is the third central moment of this spectrum, divided by the 1.5 power of the second central moment.</br>
					<br>[11]<b>Spectral Slope</b> is a measure of how quickly the spectrum of an audio sound tails off towards the high frequencies. One way to quantify this is by applying linear regression to the Fourier magnitude spectrum of the signal, which produces a single number indicating the slope of the line-of-best-fit through the spectral data.</br>
					<br>[12]<b>The mel-frequency cepstrum (MFC)</b> is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. The difference between the normal cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. This frequency warping can allow for better representation of sound</br>
					<br>Step by Step Explanation of MFCC Feature Extraction-<br>
1.	Frame the signal into short frames.<br>
2.	For each frame calculate the periodogram estimate of the power spectrum.<br>
3.	Apply the mel filterbank to the power spectra, sum the energy in each filter.<br>
4.	Take the logarithm of all filterbank energies.<br>
5.	Take the DCT of the log filterbank energies.<br>
6.	Keep DCT coefficients 2-13, discard the rest.<br>
	<br><b>Intuitive Understanding of Each step in MFCC feature extraction –</b><br>
An audio signal is constantly changing, so to simplify things we assume that on short time scales the audio signal doesn't change much (when we say it doesn't change, we mean statistically i.e. statistically stationary, obviously the samples are constantly changing on even short time scales). This is why we frame the signal into 20-40ms frames. If the frame is much shorter we don't have enough samples to get a reliable spectral estimate, if it is longer the signal changes too much throughout the frame.
The next step is to calculate the power spectrum of each frame. This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. Depending on the location in the cochlea that vibrates (which wobbles small hairs), different nerves fire informing the brain that certain frequencies are present. Our periodogram estimate performs a similar job for us, identifying which frequencies are present in the frame.
The periodogram spectral estimate still contains a lot of information not required for Automatic Speech Recognition (ASR). In particular the cochlea can not discern the difference between two closely spaced frequencies. This effect becomes more pronounced as the frequencies increase. For this reason we take clumps of periodogram bins and sum them up to get an idea of how much energy exists in various frequency regions. This is performed by our Mel filterbank: the first filter is very narrow and gives an indication of how much energy exists near 0 Hertz. As the frequencies get higher our filters get wider as we become less concerned about variations. We are only interested in roughly how much energy occurs at each spot. The Mel scale tells us exactly how to space our filterbanks and how wide to make them. 
Once we have the filterbank energies, we take the logarithm of them. This is also motivated by human hearing: we don't hear loudness on a linear scale. Generally to double the percieved volume of a sound we need to put 8 times as much energy into it. This means that large variations in energy may not sound all that different if the sound is loud to begin with. This compression operation makes our features match more closely what humans actually hear. Why the logarithm and not a cube root? The logarithm allows us to use cepstral mean subtraction, which is a channel normalisation technique.
The final step is to compute the DCT of the log filterbank energies. There are 2 main reasons this is performed. Because our filterbanks are all overlapping, the filterbank energies are quite correlated with each other. The DCT decorrelates the energies which means diagonal covariance matrices can be used to model the features in e.g. a HMM classifier. But notice that only 12 of the 26 DCT coefficients are kept. This is because the higher DCT coefficients represent fast changes in the filterbank energies and it turns out that these fast changes actually degrade ASR performance, so we get a small improvement by dropping them.<br>


					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="heading">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						<br>The dataset chosen for the task at hand is the audio samples taken from the University of Iowa - Electronic Music Studios .
						The samples taken belong to the three instrument families namely - String , Woodwind and Brass. There are a total of
						417 samples of which a 80 % - 20 % division has been done to be used for training and testing, respectively.<br>
						<br> The link to the dataset is here : <a href="http://theremin.music.uiowa.edu/MISPost2012Intro.html"> University of Iowa : Audio Samples Site</a>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						[1] In the first stage of the experiment, A neural network classifier was trained using three basic features i.e. Zero Crossing Rate,
						Spectral Centroid and Spectral RollOff Frequency , for identifying the instrument family.<br>
						The neural network was trained with dropout, and Adam Optimizer was used in training the classifier. The classifier was trained for 500 epochs and the respective training and 
						testing scores were evaluated.<br>
						<ul>
							<li>Training Accuracy : 84.73 %</li>
							<li>Testing Accuracy :   80.72 %</li>
						</ul>
						The classifier can be further improved using more number of relevant features, which we are currently working on. Also, the size of the training
						dataset is insufficient as of now. Increasing the sample size will improve the performance of the classifier greatly.<br>
						<br>Here is a snapshot of the classifier indicating training and testing accuracies :
						<br><br><img src="Picture2.png" alt="This text displays when the image is umavailable" width="500px" height=""/>
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="heading">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

		</div>
	</body>
</html>
